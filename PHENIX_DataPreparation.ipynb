{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phenix - Data preparation\n",
    "The objective of this notebook is to:\n",
    "1. [Download Phenix DB tables into csv files](#1.-Download-Phenix-DB-tables-into-csv-files)\n",
    "9. [Add food group to each Products (Product table)](#2.-Add-food-group-to-each-Products)\n",
    "9. [Add unit quantity to each Products (Product table)](#3.-Add-unit-quantity-to-each-Products)\n",
    "9. [Correct gps coordinates for each accounts (Comptes table)](#4.-Correct-gps-coordinates-for-each-Accounts)\n",
    "9. [Save the corrected tables into csv files](#5.-Save-the-corrected-tables-into-csv-files)\n",
    "\n",
    "The script requires {phenix_mgt.py} and {off_mgt.py}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phenix_mgt as phnx\n",
    "import off_mgt as off\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "phnx = reload(phnx)\n",
    "off = reload(off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download Phenix DB tables into csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this only once to save SQL table into CSV files\n",
    "# You will need to install Phenix SQL DB (.bak) into your local computer\n",
    "#phnx.extract_Phenix_SQLtable2CSV(path='./Phenix_DB/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Phenix tables\n",
    "lst_table = ['CategorieProduits',\n",
    "            'CommandeProduits',\n",
    "            'Commandes',\n",
    "            'Comptes',\n",
    "            'OffreProduits',\n",
    "            'Offres',\n",
    "            'Produits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import each table one by one\n",
    "# 'Comptes' table is imported twice for both 'Emetteur' (EC_) and 'Receveur' (RC_)\n",
    "df_catP = phnx.get_CategorieProduits('./Phenix_DB/' + lst_table[0] + '.csv', prefix='catP_')\n",
    "df_CP   = phnx.get_CommandeProduits ('./Phenix_DB/' + lst_table[1] + '.csv', prefix='CP_')\n",
    "df_CO   = phnx.get_Commandes        ('./Phenix_DB/' + lst_table[2] + '.csv', prefix='CO_')\n",
    "df_EC   = phnx.get_Comptes          ('./Phenix_DB/' + lst_table[3] + '.csv', prefix='EC_')\n",
    "df_RC   = phnx.get_Comptes          ('./Phenix_DB/' + lst_table[3] + '.csv', prefix='RC_')\n",
    "df_OP   = phnx.get_OffreProduits    ('./Phenix_DB/' + lst_table[4] + '.csv', prefix='OP_')\n",
    "df_O    = phnx.get_Offres           ('./Phenix_DB/' + lst_table[5] + '.csv', prefix='O_')\n",
    "df_P    = phnx.get_Produits         ('./Phenix_DB/' + lst_table[6] + '.csv', prefix='P_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add food group to each Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Adding Product food group column --\n",
      "--> COMPLETED \n"
     ]
    }
   ],
   "source": [
    "df_P = phnx.add_foodgroup(df_P,csv_path='./OFF_DB/mehdi_phenix_foodgroup.csv')\n",
    "df_P.to_csv('./Phenix_DB_clean/Produits.csv',sep='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add unit quantity to each Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Adding Product quantity column --\n",
      "WARNING - This function is time consuming.\n",
      "STEP 1 - 431022 (not std) 515590 (std)\n"
     ]
    }
   ],
   "source": [
    "phnx = reload(phnx)\n",
    "off = reload(off)\n",
    "df_P = phnx.add_product_qty(df_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2 - 430808 (not std) 515804 (std)\n"
     ]
    }
   ],
   "source": [
    "# STEP 2\n",
    "df_P = phnx.get_qty_from_PhenixCol(df_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> MERGE STEP ---->\n",
      "['P_Id', 'P_food_group', 'Qty_val', 'Qty_unit', 'Qty_std', 'Qty_approx', 'Qty_method']\n",
      "430808\n",
      "----> MERGE STEP ---->\n",
      "['P_Id', 'P_food_group', 'Qty_val', 'Qty_unit', 'Qty_std', 'Qty_approx', 'Qty_method', 'OP_Id', 'OP_QuantiteValeur', 'OP_QuantiteUnite']\n",
      "749397\n",
      "----> MERGE STEP ---->\n",
      "['P_Id', 'P_food_group', 'Qty_val', 'Qty_unit', 'Qty_std', 'Qty_approx', 'Qty_method', 'OP_QuantiteValeur', 'OP_QuantiteUnite', 'CP_Id', 'CP_QuantiteValeur', 'CP_QuantiteTotale', 'CP_QuantiteUnite', 'CP_MontantTotal']\n",
      "745620\n",
      "STEP 3 - 333223 (not std) 613389 (std)\n"
     ]
    }
   ],
   "source": [
    "# STEP 3\n",
    "df_P = phnx.extrapolate_qty_from_productvalue(df_P,df_OP,df_CP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4 - 313187 (not std) 633425 (std)\n"
     ]
    }
   ],
   "source": [
    "# STEP 4\n",
    "OFF_csv = './OFF_DB/fr.openfoodfacts.org.products.csv'\n",
    "df_P = phnx.get_qty_from_OFF(df_P,OFF_csv=OFF_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Correct gps coordinates for each Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Geotag Comptes table\n",
    "# df_geo_matrix is created once from IGN API and saved for later use\n",
    "###################################\n",
    "#df_geo_matrix = phnx.geotag_Compte(df_EC,prefix='EC_',IGN_API='YOUR_API_KEY')\n",
    "#df_geo_matrix.to_csv('./Phenix_DB/df_geo_matrix.csv',sep='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 coordinates has been updated of 2519\n",
      "58 coordinates has been updated of 2519\n"
     ]
    }
   ],
   "source": [
    "# Manually correct geo coordinates (function manual_geotag())\n",
    "df_geo_matrix = pd.read_csv('./Phenix_DB/df_geo_matrix.csv',sep='\\t',encoding='utf-8',index_col=0)\n",
    "df_geo_matrix = phnx.manual_geotag(df_geo_matrix)\n",
    "\n",
    "# Apply correction into df_EC et df_RC\n",
    "df_EC_corr = phnx.replace_geotag(df_EC,df_geo_matrix,prefix='EC_', threshold=1)\n",
    "df_RC_corr = phnx.replace_geotag(df_RC,df_geo_matrix,prefix='RC_', threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--DATAFRAME SIZES--\n",
      "Commande : 64945\n",
      "Com prod : 3676721\n",
      "Cat prod : 20\n",
      "Offres   : 65522\n",
      "Off prod : 3651709\n",
      "Produits : 946612\n",
      "E_Compte : 2519\n",
      "R_Compte : 2519\n",
      "\n",
      "--Produit ID--\n",
      "946612\n",
      "df_P : MAX #1040282 / MIN #1\n",
      "df_OP: MAX #1040282 / MIN #302\n"
     ]
    }
   ],
   "source": [
    "# Basic stats of tables\n",
    "print('--DATAFRAME SIZES--')\n",
    "print('Commande : %d' % len(df_CO))\n",
    "print('Com prod : %d' % len(df_CP))\n",
    "print('Cat prod : %d' % len(df_catP))\n",
    "print('Offres   : %d' % len(df_O))\n",
    "print('Off prod : %d' % len(df_OP))\n",
    "print('Produits : %d' % len(df_P))\n",
    "print('E_Compte : %d' % len(df_EC))\n",
    "print('R_Compte : %d' % len(df_RC))\n",
    "print('')\n",
    "print('--Produit ID--')\n",
    "print(len(df_P))\n",
    "print('df_P : MAX #%d / MIN #%d' % (df_P.P_Id.max(), df_P.P_Id.min()))\n",
    "print('df_OP: MAX #%d / MIN #%d' % (df_OP.Produit_Id.max(), df_OP.Produit_Id.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save the corrected tables into csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P.reset_index(drop=True,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phenix DB Tables are cleaned and ready for analysis...\n"
     ]
    }
   ],
   "source": [
    "phenix_clean_path = './Phenix_DB_clean/'\n",
    "df_catP.to_csv(phenix_clean_path + lst_table[0] + '.csv',sep='\\t',encoding='utf-8')\n",
    "df_CP.to_csv  (phenix_clean_path + lst_table[1] + '.csv',sep='\\t',encoding='utf-8')\n",
    "df_CO.to_csv  (phenix_clean_path + lst_table[2] + '.csv',sep='\\t',encoding='utf-8')\n",
    "df_EC.to_csv  (phenix_clean_path + 'EC' + lst_table[3] + '.csv',sep='\\t',encoding='utf-8')\n",
    "df_RC.to_csv  (phenix_clean_path + 'RC' + lst_table[3] + '.csv',sep='\\t',encoding='utf-8')\n",
    "df_OP.to_csv  (phenix_clean_path + lst_table[4] + '.csv',sep='\\t',encoding='utf-8')\n",
    "df_O.to_csv   (phenix_clean_path + lst_table[5] + '.csv',sep='\\t',encoding='utf-8')\n",
    "df_P.to_csv   (phenix_clean_path + lst_table[6] + '.csv',sep='\\t',encoding='utf-8')\n",
    "print('Phenix DB Tables are cleaned and ready for analysis...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
